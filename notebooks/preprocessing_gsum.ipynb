{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Contains code to generate training data for GSUM\"\"\"\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "from fuzzywuzzy import fuzz\n",
    "from typing import Any, Dict, List, Set, Tuple\n",
    "\n",
    "\n",
    "def group_entities(entities: List[Any]) -> List[Any]:\n",
    "    adj_list: Dict[Tuple[str, str], Set] = {\n",
    "        (entity[\"surface\"], entity[\"type\"]): set() for entity in entities\n",
    "    }\n",
    "\n",
    "    # Constructs a graph where a node represents an entity and an edge represents if the connected entities are the same.\n",
    "    for outer_js in entities:\n",
    "        outer = (outer_js[\"surface\"], outer_js[\"type\"])\n",
    "        for inner_js in entities:\n",
    "            inner = (inner_js[\"surface\"], inner_js[\"type\"])\n",
    "            # Substring matching to determine if an edge should be added between the nodes.\n",
    "            if (\n",
    "                outer != inner\n",
    "                and outer[1] == inner[1]\n",
    "                and (outer[0] in inner[0] or inner[0] in outer[0])\n",
    "            ):\n",
    "                adj_list[outer].add(inner)\n",
    "\n",
    "    # BFS to get groups of entities\n",
    "    queue = []\n",
    "    clusters = []\n",
    "    visited = {node: False for node in adj_list}\n",
    "    for node in list(adj_list.keys()):\n",
    "        if not visited[node]:\n",
    "            cluster = set()\n",
    "            queue.append(node)\n",
    "            while queue:\n",
    "                node = queue.pop(0)\n",
    "                visited[node] = True\n",
    "                cluster.add(node)\n",
    "                for neighbor in adj_list[node]:\n",
    "                    if not visited[neighbor]:\n",
    "                        queue.append(neighbor)\n",
    "\n",
    "            clusters.append(tuple(cluster))\n",
    "    return list(set(clusters))\n",
    "\n",
    "\n",
    "def group_entities_wrapper(entities, sent_mapping):    \n",
    "    entity_clusters = group_entities(entities)\n",
    "    final_entities = []\n",
    "    for cluster in entity_clusters:\n",
    "        cluster = list(cluster)\n",
    "        entity = {\n",
    "             \"sentences\": set(),\n",
    "             \"surface\": [c[0] for c in cluster],\n",
    "             \"type\": cluster[0][1],\n",
    "             \"max_surface\": max([c[0] for c in cluster], key=len)\n",
    "        }\n",
    "        for org_entity in entities:\n",
    "            key = (org_entity[\"surface\"], org_entity[\"type\"])\n",
    "            if key in cluster:\n",
    "                st = org_entity[\"startCharOffset\"]\n",
    "                en = org_entity[\"endCharOffset\"]\n",
    "                entity[\"sentences\"].add(sent_mapping[org_entity[\"startCharOffset\"]])\n",
    "        entity[\"sentences\"] = list(entity[\"sentences\"])\n",
    "        final_entities.append(entity) \n",
    "    return final_entities\n",
    "\n",
    "\n",
    "def extract_entity_sentences(fname):\n",
    "    all_info = [json.loads(line.strip()) for line in open(fname)]\n",
    "\n",
    "    extract = 0\n",
    "    final_entities_count, init_entities_count = 0, 0 \n",
    "    abs_final_entities_count, abs_init_entities_count = 0, 0 \n",
    "    for task_info in all_info:\n",
    "\n",
    "        sent_mapping = []\n",
    "        for ind, sent in enumerate(task_info[\"article_text_sentences\"]):\n",
    "            if ind == 0:\n",
    "                sent_mapping.extend([ind] * len(sent))\n",
    "            else:\n",
    "                sent_mapping.extend([ind] * (len(sent) + 1))\n",
    "            \n",
    "        assert len(sent_mapping) >= len(task_info[\"article_text\"])\n",
    "\n",
    "        entities = task_info[\"article_text_entities\"] \n",
    "        final_entities = group_entities_wrapper(entities, sent_mapping)\n",
    "        task_info[\"article_text_entities\"] = final_entities\n",
    "\n",
    "        init_entities_count += len(entities)\n",
    "        final_entities_count += len(final_entities)\n",
    "        if extract % 5000 == 0:\n",
    "            print(\"Entity groups:\", extract)\n",
    "        extract += 1\n",
    "\n",
    "    print(init_entities_count, final_entities_count, final_entities_count * 100.0 / init_entities_count)\n",
    "    return all_info\n",
    "\n",
    "\n",
    "def convert_mention(mention, output, comb_text):\n",
    "    start = output['subtoken_map'][mention[0]]\n",
    "    end = output['subtoken_map'][mention[1]] + 1\n",
    "    nmention = (start, end)\n",
    "    mtext = ''.join(' '.join(comb_text[mention[0]:mention[1]+1]).split(\" ##\"))\n",
    "    return (nmention, mtext)\n",
    "\n",
    "\n",
    "def get_spanbert_clusters(fname):\n",
    "    docs = set()\n",
    "    spanbert_clusters = {}\n",
    "    for line in open(fname):\n",
    "        coref_out = json.loads(line.strip())\n",
    "\n",
    "        comb_text = [word for sentence in coref_out['sentences'] for word in sentence]\n",
    "        sent_numbers = coref_out[\"sentence_map\"]\n",
    "\n",
    "        clusters = []\n",
    "        assert 'predicted_clusters' in coref_out\n",
    "        for cluster in coref_out['predicted_clusters']:\n",
    "            mapped_text, mapped_sents = set(), set()\n",
    "            for mention in cluster:\n",
    "                _, text = convert_mention(mention, coref_out, comb_text)\n",
    "                assert sent_numbers[mention[0]] == sent_numbers[mention[1]]\n",
    "                mapped_text.add(text)\n",
    "                mapped_sents.add(sent_numbers[mention[0]])\n",
    "\n",
    "            clusters.append((mapped_sents, mapped_text))\n",
    "\n",
    "        assert coref_out[\"doc_id\"] not in docs\n",
    "        docs.add(coref_out[\"doc_id\"])\n",
    "        spanbert_clusters[coref_out[\"doc_id\"]] = clusters\n",
    "        \n",
    "    return spanbert_clusters\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate CNNDM / NYT data for GSUM experiments that involved training one entity at a time.\"\"\"\n",
    "\n",
    "\n",
    "def generate_data(ner_article, ner_abstract_fname, out_dir, coref_article, coref_abstract, guidance):\n",
    "    \n",
    "    all_info = extract_entity_sentences(ner_article)\n",
    "    random.shuffle(all_info)\n",
    "    \n",
    "    all_article_clusters = get_spanbert_clusters(coref_article)\n",
    "    all_abstract_clusters = get_spanbert_clusters(coref_abstract)\n",
    "    print(\"Coreferenced documents: \", len(all_article_clusters), len(all_abstract_clusters))\n",
    "    print(\"Input file size: \", len(all_info))\n",
    "    \n",
    "    ind = 0\n",
    "    with open(out_dir + \".source\", 'w') as fps, open(out_dir + \".target\", 'w') as  fpd, open(out_dir + \".z\", 'w') as fpz:\n",
    "        done = set()\n",
    "        abstract_num = 0\n",
    "        for ind, task_info in enumerate(all_info):\n",
    "            doc_id = int(task_info[\"doc_id\"])\n",
    "            assert doc_id not in done\n",
    "            done.add(doc_id)\n",
    "            \n",
    "            entities = task_info['article_text_entities']\n",
    "            doc_sents = task_info['article_text_sentences']\n",
    "            abstract_sents = task_info['abstract_text_sentences']\n",
    "            article_clusters = all_article_clusters[doc_id] if doc_id in all_article_clusters else []\n",
    "            abstract_clusters = all_abstract_clusters[doc_id] if doc_id in all_abstract_clusters else []\n",
    "\n",
    "            for eind, entity in enumerate(entities): \n",
    "                \n",
    "                if entity[\"max_surface\"] != \"CNN\":\n",
    "\n",
    "                    # Sentences in the reference containing the entity and its coreferences.\n",
    "                    abstract_sent_ids = set()\n",
    "                    for cluster in abstract_clusters:\n",
    "                        if any([ename in name for name in cluster[1] for ename in entity[\"surface\"]]):\n",
    "                            abstract_sent_ids.update(list(cluster[0]))\n",
    "                    for sent_id, sent in enumerate(abstract_sents):\n",
    "                        if any([surface in sent and len(surface) > 5 for surface in entity[\"surface\"]]):\n",
    "                                abstract_sent_ids.add(sent_id)               \n",
    "                    abstract_sent_ids = sorted(list(abstract_sent_ids))\n",
    "                    \n",
    "                    if len(abstract_sent_ids) > 0:\n",
    "                        abstract_num += 1\n",
    "\n",
    "                    # Sentences in the source document containing the entity and its coreferences.\n",
    "                    article_sent_ids = entity[\"sentences\"][:]\n",
    "                    for cluster in article_clusters:\n",
    "                        if any([ename in name for name in cluster[1] for ename in entity[\"surface\"]]):\n",
    "                            article_sent_ids.extend(list(cluster[0]))\n",
    "                    article_sent_ids = sorted(list(set(article_sent_ids)))\n",
    "\n",
    "                    # Get upto 3 sentences from the reference that mention the entity. Else take the lead3 of the document.\n",
    "                    final_sents = []\n",
    "                    for idx in abstract_sent_ids:\n",
    "                        if len(final_sents) < 3:\n",
    "                            final_sents.append(abstract_sents[idx])\n",
    "\n",
    "                    for idx in article_sent_ids:\n",
    "                        if len(final_sents) < 3 and fuzz.ratio(\" \".join(final_sents), doc_sents[idx]) < 60:\n",
    "                            final_sents.append(doc_sents[idx])\n",
    "                        \n",
    "                    assert 0 < len(final_sents) <= 3\n",
    "                    final_sents_text = \" \".join(final_sents)\n",
    "                    if len(final_sents) > 0 and \" . \" not in final_sents_text:\n",
    "                        fps.write(\" \".join(doc_sents) + \"\\n\")\n",
    "                        if guidance == \"entity\":\n",
    "                            fpz.write(\" | \".join(entity[\"surface\"]) + \"\\n\")\n",
    "                        elif guidance == \"lead3\":\n",
    "                            fpz.write(\" \".join([doc_sents[idx] for idx in article_sent_ids[:3]]) + \"\\n\")\n",
    "                        else:\n",
    "                            fpz.write(\" \".join([doc_sents[idx] for idx in article_sent_ids]) + \"\\n\")\n",
    "                        fpd.write(\" \".join(final_sents) + \"\\n\")         \n",
    "                \n",
    "            if ind % 5000 == 0:\n",
    "                print(ind)\n",
    "                print(entity[\"surface\"])\n",
    "                print(final_sents)\n",
    "                if len(abstract_sent_ids) > 0:\n",
    "                    print(doc_sents[abstract_sent_ids[0]])\n",
    "                print()\n",
    "                \n",
    "        print(abstract_num)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    GUIDANCE = \"entity\"\n",
    "    TYPE = \"one_entity_name_guidance\"\n",
    "    \n",
    "    generate_data(\"cnndm/ner/train.jsonl\", \n",
    "                  \"cnndm/ner/train.jsonl\",\n",
    "                  \"cnndm/gsum/\" + TYPE + \"/raw/train\", \n",
    "                  \"cnndm/coref/output/train_article.jsonl\", \n",
    "                  \"cnndm/coref/output/train_abstract.jsonl\", GUIDANCE)\n",
    "    \n",
    "    generate_data(\"cnndm/ner/val.jsonl\", \n",
    "                  \"cnndm/ner/val.jsonl\",\n",
    "                  \"cnndm/gsum/\" + TYPE + \"/raw/val\", \n",
    "                  \"cnndm/coref/output/val_article.jsonl\", \n",
    "                  \"cnndm/coref/output/val_abstract.jsonl\", GUIDANCE)\n",
    "    \n",
    "    generate_data(\"cnndm/ner/val.jsonl\", \n",
    "                  \"cnndm/ner/val.jsonl\",\n",
    "                  \"cnndm/gsum/\" + TYPE + \"/raw/test\", \n",
    "                  \"cnndm/coref/output/val_article.jsonl\", \n",
    "                  \"cnndm/coref/output/val_abstract.jsonl\", GUIDANCE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
