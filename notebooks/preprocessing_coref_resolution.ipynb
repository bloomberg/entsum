{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "## Coreference resolution\n",
    "############################################\n",
    "\n",
    "# Step - 1: Installation instructions.\n",
    "######################################\n",
    "# 1. Download the code from https://github.com/mandarjoshi90/coref and unzip it.\n",
    "# 2. Download spanbert large checkpoint from the same link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step - 2: Necessary installations\n",
    "######################################\n",
    "\n",
    "%cd coref-master\n",
    "\n",
    "! pip install pyhocon\n",
    "! pip install torch\n",
    "\n",
    "! sed -i 's/-D_GLIBCXX_USE_CXX11_ABI=0//' setup_all.sh\n",
    "! ./setup_all.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step - 3: Setup env variables\n",
    "######################################\n",
    "\n",
    "genre = \"nw\"\n",
    "model_name = \"spanbert_large\"\n",
    "\n",
    "import os\n",
    "os.environ['data_dir'] = \".\"\n",
    "os.environ['CHOSEN_MODEL'] = model_name\n",
    "\n",
    "# Determine Max Segment\n",
    "max_segment = None\n",
    "for line in open('experiments.conf'):\n",
    "    if line.startswith(model_name):\n",
    "        max_segment = True\n",
    "    elif line.strip().startswith(\"max_segment_len\"):\n",
    "        if max_segment:\n",
    "            max_segment = int(line.strip().split()[-1])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Convert raw input text to SpanBERT format\n",
    "####################################################\n",
    "\n",
    "from bert import tokenization\n",
    "import json\n",
    "\n",
    "example_text = [\n",
    "\"Acknowledging the Hurricane Katrina failures, the Bush administration advocated giving federal agencies from the Pentagon to the Department of Justice a greater role in the nation's disaster response playbook.\",\n",
    "\"If adopted through both legislation and executive order, the recommendations would reverse some of the steps taken after the Sept. 11 terrorist attacks to centralize responsibility for responding to natural disasters or terrorist attacks at the newly created Department of Homeland Security.\",\n",
    "\"And the plan could require the White House to play a larger coordinating role in future disasters.\",\n",
    "\"Frances Fragos Townsend, President Bush's domestic security adviser, said that enlisting help from federal agencies made sense.\",\n",
    "\"But some critics worry that diffusing responsibilities among agencies could leave no one clearly in charge and not produce results.\", \n",
    "\"''This may simply be rearranging the deck chairs on the Titanic,'' said Michael Greenberger, a law professor at the University of Maryland.\", \n",
    "\"The Homeland Security Department and its Federal Emergency Management Agency will continue to be the lead federal player in disaster response efforts, according to the blueprint proposed by Ms. Townsend.\",\n",
    "]\n",
    "\n",
    "fout = open(\"sample.jsonl\", 'w')\n",
    "\n",
    "index = 0\n",
    "for json_str in json_list:\n",
    "    result = json.loads(json_str)\n",
    "    text = example_text\n",
    "    \n",
    "    data = {\n",
    "        'doc_id': result['id'],\n",
    "        'doc_key': \"nw\",\n",
    "        'sentences': [[\"[CLS]\"]],\n",
    "        'speakers': [[\"[SPL]\"]],\n",
    "        'clusters': [],\n",
    "        'sentence_map': [0],\n",
    "        'subtoken_map': [0],\n",
    "    }\n",
    "\n",
    "\n",
    "    tokenizer = tokenization.FullTokenizer(vocab_file=\"cased_config_vocab/vocab.txt\", do_lower_case=False)\n",
    "    subtoken_num = 0\n",
    "    for sent_num, line in enumerate(text):\n",
    "        \n",
    "        if len(line) == 0:\n",
    "            line = \".\"\n",
    "        \n",
    "        raw_tokens = line.split()\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        if len(tokens) + len(data['sentences'][-1]) >= max_segment:\n",
    "            data['sentences'][-1].append(\"[SEP]\")\n",
    "            data['sentences'].append([\"[CLS]\"])\n",
    "            data['speakers'][-1].append(\"[SPL]\")\n",
    "            data['speakers'].append([\"[SPL]\"])\n",
    "            data['sentence_map'].append(sent_num - 1)\n",
    "            data['subtoken_map'].append(subtoken_num - 1)\n",
    "            data['sentence_map'].append(sent_num)\n",
    "            data['subtoken_map'].append(subtoken_num)\n",
    "            \n",
    "        ctoken = raw_tokens[0]\n",
    "        cpos = 0\n",
    "        for token in tokens:\n",
    "            data['sentences'][-1].append(token)\n",
    "            data['speakers'][-1].append(\"-\")\n",
    "            data['sentence_map'].append(sent_num)\n",
    "            data['subtoken_map'].append(subtoken_num)\n",
    "\n",
    "            if token.startswith(\"##\"):\n",
    "                token = token[2:]\n",
    "            if len(ctoken) == len(token):\n",
    "                subtoken_num += 1\n",
    "                cpos += 1\n",
    "                if cpos < len(raw_tokens):\n",
    "                    ctoken = raw_tokens[cpos]\n",
    "            else:\n",
    "                ctoken = ctoken[len(token):]\n",
    "\n",
    "    data['sentences'][-1].append(\"[SEP]\")\n",
    "    data['speakers'][-1].append(\"[SPL]\")\n",
    "    data['sentence_map'].append(sent_num - 1)\n",
    "    data['subtoken_map'].append(subtoken_num - 1)\n",
    "\n",
    "    json.dump(data, fout, sort_keys=True)\n",
    "    fout.write('\\n')\n",
    "    \n",
    "    if index % 100 == 0:\n",
    "        print(index, \"samples processed.\")\n",
    "    index += 1\n",
    "    \n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!GPU=0 python3 predict.py $CHOSEN_MODEL sample.jsonl sample_out.jsonl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
