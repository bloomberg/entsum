{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess training / val dataset for CNNDM \n",
    "\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def group_entities(entities: List[Any]) -> List[Any]:\n",
    "    adj_list: Dict[Tuple[str, str], Set] = {\n",
    "        (entity[\"surface\"], entity[\"type\"]): set() for entity in entities\n",
    "    }\n",
    "\n",
    "    # Constructs a graph where a node represents an entity and an edge represents if the connected entities are the same.\n",
    "    for outer_js in entities:\n",
    "        outer = (outer_js[\"surface\"], outer_js[\"type\"])\n",
    "        for inner_js in entities:\n",
    "            inner = (inner_js[\"surface\"], inner_js[\"type\"])\n",
    "            # Substring matching to determine if an edge should be added between the nodes.\n",
    "            if (\n",
    "                outer != inner\n",
    "                and outer[1] == inner[1]\n",
    "                and (outer[0] in inner[0] or inner[0] in outer[0])\n",
    "            ):\n",
    "                adj_list[outer].add(inner)\n",
    "\n",
    "    # BFS to get groups of entities\n",
    "    queue = []\n",
    "    clusters = []\n",
    "    visited = {node: False for node in adj_list}\n",
    "    for node in list(adj_list.keys()):\n",
    "        if not visited[node]:\n",
    "            cluster = set()\n",
    "            queue.append(node)\n",
    "            while queue:\n",
    "                node = queue.pop(0)\n",
    "                visited[node] = True\n",
    "                cluster.add(node)\n",
    "                for neighbor in adj_list[node]:\n",
    "                    if not visited[neighbor]:\n",
    "                        queue.append(neighbor)\n",
    "\n",
    "            clusters.append(tuple(cluster))\n",
    "    return list(set(clusters))\n",
    "\n",
    "\n",
    "def group_entities_wrapper(entities, sent_mapping):    \n",
    "    entity_clusters = group_entities(entities)\n",
    "    final_entities = []\n",
    "    for cluster in entity_clusters:\n",
    "        cluster = list(cluster)\n",
    "        entity = {\n",
    "             \"sentences\": set(),\n",
    "             \"surface\": [c[0] for c in cluster],\n",
    "             \"type\": cluster[0][1],\n",
    "             \"max_surface\": max([c[0] for c in cluster], key=len)\n",
    "        }\n",
    "        for org_entity in entities:\n",
    "            key = (org_entity[\"surface\"], org_entity[\"type\"])\n",
    "            if key in cluster:\n",
    "                st = org_entity[\"startCharOffset\"]\n",
    "                en = org_entity[\"endCharOffset\"]\n",
    "                entity[\"sentences\"].add(sent_mapping[org_entity[\"startCharOffset\"]])\n",
    "        entity[\"sentences\"] = list(entity[\"sentences\"])\n",
    "        final_entities.append(entity) \n",
    "    return final_entities\n",
    "\n",
    "\n",
    "def extract_entity_sentences(fname):\n",
    "    all_info = [json.loads(line.strip()) for line in open(fname)]\n",
    "\n",
    "    extract = 0\n",
    "    final_entities_count, init_entities_count = 0, 0 \n",
    "    abs_final_entities_count, abs_init_entities_count = 0, 0 \n",
    "    for task_info in all_info:\n",
    "\n",
    "        sent_mapping = []\n",
    "        for ind, sent in enumerate(task_info[\"article_text_sentences\"]):\n",
    "            if ind == 0:\n",
    "                sent_mapping.extend([ind] * len(sent))\n",
    "            else:\n",
    "                sent_mapping.extend([ind] * (len(sent) + 1))\n",
    "            \n",
    "        assert len(sent_mapping) >= len(task_info[\"article_text\"])\n",
    "\n",
    "        entities = task_info[\"article_text_entities\"] \n",
    "        final_entities = group_entities_wrapper(entities, sent_mapping)\n",
    "        task_info[\"article_text_entities\"] = final_entities\n",
    "\n",
    "        init_entities_count += len(entities)\n",
    "        final_entities_count += len(final_entities)\n",
    "        if extract % 5000 == 0:\n",
    "            print(\"Entity groups:\", extract)\n",
    "        extract += 1\n",
    "\n",
    "    print(init_entities_count, final_entities_count, final_entities_count * 100.0 / init_entities_count)\n",
    "    return all_info\n",
    "\n",
    "\n",
    "def convert_mention(mention, output, comb_text):\n",
    "    start = output['subtoken_map'][mention[0]]\n",
    "    end = output['subtoken_map'][mention[1]] + 1\n",
    "    nmention = (start, end)\n",
    "    mtext = ''.join(' '.join(comb_text[mention[0]:mention[1]+1]).split(\" ##\"))\n",
    "    return (nmention, mtext)\n",
    "\n",
    "\n",
    "def get_spanbert_clusters(fname):\n",
    "    docs = set()\n",
    "    spanbert_clusters = {}\n",
    "    abc = 0\n",
    "    for line in open(fname):\n",
    "        coref_out = json.loads(line.strip())\n",
    "\n",
    "        comb_text = [word for sentence in coref_out['sentences'] for word in sentence]\n",
    "        sent_numbers = coref_out[\"sentence_map\"]\n",
    "\n",
    "        clusters = []\n",
    "        assert 'predicted_clusters' in coref_out\n",
    "        for cluster in coref_out['predicted_clusters']:\n",
    "            mapped_text, mapped_sents = set(), set()\n",
    "            for mention in cluster:\n",
    "                _, text = convert_mention(mention, coref_out, comb_text)\n",
    "                assert sent_numbers[mention[0]] == sent_numbers[mention[1]]\n",
    "                mapped_text.add(text)\n",
    "                mapped_sents.add(sent_numbers[mention[0]])\n",
    "\n",
    "            clusters.append((mapped_sents, mapped_text))\n",
    "\n",
    "        assert coref_out[\"doc_id\"] not in docs\n",
    "        docs.add(coref_out[\"doc_id\"])\n",
    "        spanbert_clusters[coref_out[\"doc_id\"]] = clusters\n",
    "        \n",
    "        if abc % 10000 == 0:\n",
    "            print(abc)\n",
    "        abc += 1\n",
    "    return spanbert_clusters\n",
    "    \n",
    "    \n",
    "def hashhex(s):\n",
    "    \"\"\"Returns a heximal formated SHA1 hash of the input string.\"\"\"\n",
    "    h = hashlib.sha1()\n",
    "    h.update(s.encode('utf-8'))\n",
    "    return h.hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(ner_article, coref_article, out_dir, out_dir_url, split):\n",
    "    \n",
    "    all_info = extract_entity_sentences(ner_article)\n",
    "    \n",
    "    all_article_clusters = get_spanbert_clusters(coref_article)\n",
    "    print(\"Coreferenced documents: \", len(all_article_clusters))\n",
    "    print(\"Input file size: \", len(all_info))\n",
    "    \n",
    "    with open(out_dir_url, 'w') as fpfiles:\n",
    "        done = set()\n",
    "        for ind, task_info in enumerate(all_info):\n",
    "            \n",
    "            doc_id = int(task_info[\"doc_id\"])\n",
    "            entities = task_info['article_text_entities']\n",
    "            doc_sents = task_info[\"article_text_sentences\"]\n",
    "            article_clusters = all_article_clusters[doc_id] if doc_id in all_article_clusters else []\n",
    "            \n",
    "            if all([len(sent) < 400 for sent in doc_sents]):\n",
    "                for eind, entity in enumerate(entities): \n",
    "                    if entity[\"max_surface\"] != \"CNN\":\n",
    "                        # Sentences in the source document containing the entity and its coreferences.\n",
    "                        article_sent_ids = entity[\"sentences\"][:]\n",
    "                        for cluster in article_clusters:\n",
    "                            if any([ename in name for name in cluster[1] for ename in entity[\"surface\"]]):\n",
    "                                article_sent_ids.extend(list(cluster[0]))\n",
    "                        article_sent_ids = sorted(list(set(article_sent_ids)))[:3]\n",
    "                    \n",
    "                        story_fname = str(ind) + \"-\" + str(eind) + \".\" + split + \".story\"\n",
    "                        story_fname_hexname = hashhex(story_fname)\n",
    "                        fpfiles.write(story_fname + \"\\n\")\n",
    "                        assert story_fname_hexname not in done\n",
    "                        done.add(story_fname_hexname)\n",
    "\n",
    "                        with open(out_dir + \"/\" + story_fname_hexname + \".story\", \"w\") as fp:\n",
    "                            fp.write(\" | \".join([name for name in entity[\"surface\"]]) + \" =>\\n\\n\")\n",
    "\n",
    "                            for sent in doc_sents:\n",
    "                                fp.write(sent.strip() + \"\\n\\n\")\n",
    "\n",
    "                            for sent_id in article_sent_ids:\n",
    "                                fp.write(\"@highlight\\n\\n\")\n",
    "                                fp.write(doc_sents[sent_id].strip() + \"\\n\\n\")\n",
    "                              \n",
    "                \n",
    "                if ind % 5000 == 0:\n",
    "                    print(ind)\n",
    "                    print(entity[\"surface\"])\n",
    "                    print([(doc_sents[sent_id], len(doc_sents[sent_id])) for sent_id in article_sent_ids])\n",
    "                    print()\n",
    "                \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_data(\"ner/val.jsonl\", \"coref/output/val_article.jsonl\", \n",
    "                  \"bertsum/lead3_sents/raw_data\",\n",
    "                  \"bertsum/lead3_sents/urls/mapping_valid.txt\", \n",
    "                  \"valid\")\n",
    "    \n",
    "    generate_data(\"ner/val.jsonl\", \"coref/output/val_article.jsonl\", \n",
    "                  \"bertsum/lead3_sents/raw_data\",\n",
    "                  \"bertsum/lead3_sents/urls/mapping_test.txt\", \n",
    "                  \"test\")\n",
    "    \n",
    "    generate_data(\"ner/train.jsonl\", \"coref/output/train_article.jsonl\", \n",
    "                  \"bertsum/lead3_sents/raw_data\",\n",
    "                  \"bertsum/lead3_sents/urls/mapping_train.txt\", \n",
    "                  \"train\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
