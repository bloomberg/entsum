{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "## Entity Tagging using Flair for NYT corpus\n",
    "############################################\n",
    "\n",
    "# Setup instructions\n",
    "# 1. Download the flair ner model \"en-ner-conll03-v0.4.pt\" to the same directory.\n",
    "# 2. Install flair: pip install flair\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from flair.models import SequenceTagger\n",
    "from flair.tokenization import SegtokSentenceSplitter\n",
    "\n",
    "# Initialize Tagger and Sentence splitter\n",
    "tagger = SequenceTagger.load('en-ner-conll03-v0.4.pt')\n",
    "splitter = SegtokSentenceSplitter()\n",
    "\n",
    "def get_tagged_field(field_name, stories):\n",
    "    type_ctr = defaultdict(int)\n",
    "    overall_story_len = 0\n",
    "    selected_stories = []\n",
    "    for idx, story_data in enumerate(stories):\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            print(idx, \" document processed.\")\n",
    "        \n",
    "        text = story_data[field_name].replace(\"\\n\\n\", \" \")\n",
    "\n",
    "        # use splitter to split text into list of sentences\n",
    "        sentences = splitter.split(text)\n",
    "\n",
    "        # predict tags for sentences\n",
    "        tagger.predict(sentences)\n",
    "\n",
    "        # iterate through sentences and print predicted labels\n",
    "        entities = []\n",
    "        tagged_sentences = []\n",
    "        sentence_text = []\n",
    "        story_len = 0\n",
    "        story_offset = 0\n",
    "        for sentence in sentences:\n",
    "            story_len += len(sentence.tokens)\n",
    "            sentence_text.append(sentence.to_original_text())\n",
    "            for entity in sentence.get_spans('ner'):\n",
    "                entity_label = entity.labels[0].value\n",
    "                if entity_label == \"ORG\" or entity_label == \"PER\":\n",
    "                    entities.append({\"surface\": entity.text,\n",
    "                                     \"type\": entity_label,\n",
    "                                     \"startCharOffset\": entity.start_pos + story_offset,\n",
    "                                     \"endCharOffset\": entity.start_pos + story_offset + len(entity.text)})\n",
    "            story_offset += len(sentence.to_original_text()) + 1\n",
    "\n",
    "        story_data[field_name] = \" \".join(sentence_text).strip()\n",
    "        story_data[field_name + \"_sentences\"] = sentence_text\n",
    "        story_data[field_name + \"_entities\"] = entities\n",
    "\n",
    "        if story_len <= 1500:\n",
    "            selected_stories.append(story_data)\n",
    "            overall_story_len += story_len\n",
    "            # Validate entities and capture statistics\n",
    "            for ent_idx, entity in enumerate(entities):\n",
    "                actual_text = story_data[field_name][entity[\"startCharOffset\"]:entity[\"endCharOffset\"]]\n",
    "                if actual_text != entity[\"surface\"]:\n",
    "                    del entities[ent_idx]\n",
    "                    continue\n",
    "                type_ctr[entity[\"type\"]] += 1\n",
    "    \n",
    "    print(\"Average Story Length: \", overall_story_len/(len(selected_stories)*1.0))\n",
    "    print(\"Entity Distribution: \", type_ctr)\n",
    "    return selected_stories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file and predict entities for NYT corpus\n",
    "\n",
    "DATA_FNAME = \"<Input jsonl file>\"\n",
    "OUT_FNAME = \"<Output jsonl file with entity information>\"\n",
    "\n",
    "stories = []\n",
    "for line in open(DATA_FNAME):\n",
    "    story_data = json.loads(line)\n",
    "    stories.append(story_data)\n",
    "print(\"Number of stories: \", len(stories))\n",
    "\n",
    "stories = get_tagged_field(\"article_text\", stories)\n",
    "print(\"Done article text\")\n",
    "stories = get_tagged_field(\"abstract_text\", stories)\n",
    "print(\"Done abstract text\")\n",
    "\n",
    "fout = open(OUT_FNAME, \"wb\")\n",
    "for story_data in stories:\n",
    "    fout.write(str.encode(json.dumps(story_data), \"utf-8\"))\n",
    "    fout.write(str.encode(\"\\n\", \"utf-8\"))\n",
    "fout.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
